{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNf07OWiQCwcpgmYPMEA+2S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumaiya008/CCNY-DSE-Capstone-Project-Segmenting-Coral-Branch-tips/blob/main/notebooks/1_2_Data_Augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NsdWTF0dIywv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import gdown\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dU3E7y5DI4DZ",
        "outputId": "3f03c60e-3dd6-4a34-835a-5fd93abdc7b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to your image folders\n",
        "image_folder_path = \"/content/drive/MyDrive/Capstone/Coral_images/\"\n",
        "\n",
        "# Initialize lists to store images and labels\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "# Iterate through each folder in the main directory\n",
        "for folder_name in os.listdir(image_folder_path):\n",
        "    folder_path = os.path.join(image_folder_path, folder_name)\n",
        "\n",
        "    # Check if the item in the main directory is a folder\n",
        "    if os.path.isdir(folder_path):\n",
        "        # Iterate through each image file in the subfolder\n",
        "        for image_filename in os.listdir(folder_path):\n",
        "            image_path = os.path.join(folder_path, image_filename)\n",
        "\n",
        "            # Load the image using OpenCV\n",
        "            image = cv2.imread(image_path)\n",
        "\n",
        "            # Check if the image was loaded successfully\n",
        "            if image is not None:\n",
        "                # Resize the image to 1080x1080 if needed\n",
        "                if image.shape[0] >= 1080 and image.shape[1] >= 1080:\n",
        "                    image = cv2.resize(image, (1080, 1080))\n",
        "                else:\n",
        "                    print(f\"Image dimensions are too small for resizing: {image_path}\")\n",
        "\n",
        "                # Append the image to the list of images\n",
        "                images.append(image)\n",
        "\n",
        "                # Append the label (folder name) to the list of labels\n",
        "                labels.append(folder_name)\n",
        "            else:\n",
        "                # Handle the case where the image couldn't be loaded\n",
        "                print(f\"Error loading image: {image_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7Wyh7jVI5gO",
        "outputId": "ecfdbaa0-c349-4a04-f6ee-115da1f34bbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading image: /content/drive/MyDrive/Capstone/Coral_images/Pseudodiploria/.DS_Store\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the lists to NumPy arrays for further analysis\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Now, check the shapes\n",
        "print(f\"Images shape: {images.shape}\")\n",
        "print(f\"Labels shape: {labels.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLz7XGhjI_4n",
        "outputId": "745b417e-da9a-480e-86ee-55e714fcc391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images shape: (907, 1080, 1080, 3)\n",
            "Labels shape: (907,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the data augmentation parameters\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    brightness_range=[0.5, 1.5],\n",
        "    fill_mode='nearest'\n",
        ")\n"
      ],
      "metadata": {
        "id": "GYW0dfh-JCQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide the data into 20 batches\n",
        "batch_size = len(images) // 20\n",
        "\n",
        "for batch_number in range(20):\n",
        "    # Define the start and end indices for the current batch\n",
        "    start_idx = batch_number * batch_size\n",
        "    end_idx = (batch_number + 1) * batch_size if batch_number < 19 else len(images)\n",
        "\n",
        "    # Extract the batch of images and labels\n",
        "    batch_images = images[start_idx:end_idx]\n",
        "    batch_labels = labels[start_idx:end_idx]\n",
        "\n",
        "    # Apply data augmentation to the batch\n",
        "    augmented_images = []\n",
        "    augmented_labels = []\n",
        "\n",
        "    for i in range(len(batch_images)):\n",
        "        img = batch_images[i]\n",
        "        label = batch_labels[i]\n",
        "\n",
        "        img = np.expand_dims(img, axis=0)\n",
        "\n",
        "        for j in range(2):\n",
        "            x_augmented = datagen.flow(img, batch_size=1).next()\n",
        "            augmented_images.append(x_augmented[0])\n",
        "            augmented_labels.append(label)\n",
        "\n",
        "    # Convert the augmented data to NumPy arrays\n",
        "    augmented_images = np.array(augmented_images)\n",
        "    augmented_labels = np.array(augmented_labels)\n",
        "\n",
        "    # Save the augmented data to a compressed NumPy file\n",
        "    save_path = f'/content/drive/MyDrive/Capstone/Coral_images/data_augmented_{batch_number}.npz'\n",
        "    np.savez_compressed(save_path, images=augmented_images, labels=augmented_labels)\n",
        "\n",
        "    print(f'Saved batch {batch_number} to {save_path}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHOSS7nEJJkG",
        "outputId": "adceb09a-09fd-4d9b-f149-03d5ca047f96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved batch 0 to /content/drive/MyDrive/Capstone/Coral_images/data_augmented_0.npz\n",
            "Saved batch 1 to /content/drive/MyDrive/Capstone/Coral_images/data_augmented_1.npz\n",
            "Saved batch 2 to /content/drive/MyDrive/Capstone/Coral_images/data_augmented_2.npz\n",
            "Saved batch 3 to /content/drive/MyDrive/Capstone/Coral_images/data_augmented_3.npz\n",
            "Saved batch 4 to /content/drive/MyDrive/Capstone/Coral_images/data_augmented_4.npz\n",
            "Saved batch 5 to /content/drive/MyDrive/Capstone/Coral_images/data_augmented_5.npz\n",
            "Saved batch 6 to /content/drive/MyDrive/Capstone/Coral_images/data_augmented_6.npz\n",
            "Saved batch 7 to /content/drive/MyDrive/Capstone/Coral_images/data_augmented_7.npz\n",
            "Saved batch 8 to /content/drive/MyDrive/Capstone/Coral_images/data_augmented_8.npz\n",
            "Saved batch 9 to /content/drive/MyDrive/Capstone/Coral_images/data_augmented_9.npz\n",
            "Saved batch 10 to /content/drive/MyDrive/Capstone/Coral_images/data_augmented_10.npz\n",
            "Saved batch 11 to /content/drive/MyDrive/Capstone/Coral_images/data_augmented_11.npz\n",
            "Saved batch 12 to /content/drive/MyDrive/Capstone/Coral_images/data_augmented_12.npz\n",
            "Saved batch 13 to /content/drive/MyDrive/Capstone/Coral_images/data_augmented_13.npz\n",
            "Saved batch 14 to /content/drive/MyDrive/Capstone/Coral_images/data_augmented_14.npz\n",
            "Saved batch 15 to /content/drive/MyDrive/Capstone/Coral_images/data_augmented_15.npz\n",
            "Saved batch 16 to /content/drive/MyDrive/Capstone/Coral_images/data_augmented_16.npz\n",
            "Saved batch 17 to /content/drive/MyDrive/Capstone/Coral_images/data_augmented_17.npz\n",
            "Saved batch 18 to /content/drive/MyDrive/Capstone/Coral_images/data_augmented_18.npz\n",
            "Saved batch 19 to /content/drive/MyDrive/Capstone/Coral_images/data_augmented_19.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Initialize a dictionary to store the number of images in each batch\n",
        "image_counts = {}\n",
        "\n",
        "# Define the path where the files are stored\n",
        "base_path = '/content/drive/MyDrive/Capstone/Coral_images/'\n",
        "\n",
        "for batch_number in range(20):\n",
        "    # Construct the file path\n",
        "    file_path = f'{base_path}data_augmented_{batch_number}.npz'\n",
        "\n",
        "    # Load the NumPy file\n",
        "    data = np.load(file_path)\n",
        "\n",
        "    # Get the images from the loaded data\n",
        "    images = data['images']\n",
        "\n",
        "    # Get the number of images in the batch\n",
        "    num_images = len(images)\n",
        "\n",
        "    # Store the count in the dictionary\n",
        "    image_counts[batch_number] = num_images\n",
        "\n",
        "    # Close the NumPy file\n",
        "    data.close()\n",
        "\n",
        "# Print the number of images in each batch\n",
        "for batch_number, count in image_counts.items():\n",
        "    print(f'Batch {batch_number}: {count} images')\n"
      ],
      "metadata": {
        "id": "aWNRJ8wyJO84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caf86402-8270-4887-b774-8eb65ffe65b1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0: 90 images\n",
            "Batch 1: 90 images\n",
            "Batch 2: 90 images\n",
            "Batch 3: 90 images\n",
            "Batch 4: 90 images\n",
            "Batch 5: 90 images\n",
            "Batch 6: 90 images\n",
            "Batch 7: 90 images\n",
            "Batch 8: 90 images\n",
            "Batch 9: 90 images\n",
            "Batch 10: 90 images\n",
            "Batch 11: 90 images\n",
            "Batch 12: 90 images\n",
            "Batch 13: 90 images\n",
            "Batch 14: 90 images\n",
            "Batch 15: 90 images\n",
            "Batch 16: 90 images\n",
            "Batch 17: 90 images\n",
            "Batch 18: 90 images\n",
            "Batch 19: 104 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "\n",
        "# Initialize an HDF5 file\n",
        "hdf5_path = '/content/drive/MyDrive/Capstone/Coral_images/all_data_augmented.h5'\n",
        "hdf5_file = h5py.File(hdf5_path, 'w')\n",
        "\n",
        "# Define the path where the files are stored\n",
        "base_path = '/content/drive/MyDrive/Capstone/Coral_images/'\n",
        "\n",
        "for batch_number in range(20):\n",
        "    # Construct the file path\n",
        "    file_path = f'{base_path}data_augmented_{batch_number}.npz'\n",
        "\n",
        "    # Load the NumPy file\n",
        "    data = np.load(file_path)\n",
        "\n",
        "    # Get the images and labels from the loaded data\n",
        "    batch_images = data['images']\n",
        "    batch_labels = data['labels']\n",
        "\n",
        "    # Create datasets in the HDF5 file for this batch\n",
        "    hdf5_file.create_dataset(f'images_{batch_number}', data=batch_images)\n",
        "    hdf5_file.create_dataset(f'labels_{batch_number}', data=batch_labels)\n",
        "\n",
        "    # Close the NumPy file\n",
        "    data.close()\n",
        "\n",
        "# Close the HDF5 file\n",
        "hdf5_file.close()\n",
        "\n",
        "# Print a confirmation message\n",
        "print(f'Saved all data to {hdf5_path}')\n"
      ],
      "metadata": {
        "id": "pI9xAlU-UaJ5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "outputId": "17971f58-fffe-431f-8c7a-dc84510698bd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-77f4fb608402>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Create datasets in the HDF5 file for this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mhdf5_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'images_{batch_number}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mhdf5_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'labels_{batch_number}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Close the NumPy file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    181\u001b[0m                     \u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mdsid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_new_dset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mtid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogical\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m# Legacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/h5t.pyx\u001b[0m in \u001b[0;36mh5py.h5t.py_create\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5t.pyx\u001b[0m in \u001b[0;36mh5py.h5t.py_create\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5t.pyx\u001b[0m in \u001b[0;36mh5py.h5t.py_create\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: No conversion path for dtype: dtype('<U14')"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Paths to the two files to be combined\n",
        "file_path_1 = '/content/drive/MyDrive/Capstone/Coral_images/data_augmented_1.npz'\n",
        "file_path_2 = '/content/drive/MyDrive/Capstone/Coral_images/data_augmented_0.npz'\n",
        "\n",
        "# Load the data from the first file and check available keys\n",
        "data_1 = np.load(file_path_1)\n",
        "print(\"Keys in file 1:\", list(data_1.keys()))\n",
        "\n",
        "# Load the data from the second file and check available keys\n",
        "data_2 = np.load(file_path_2)\n",
        "print(\"Keys in file 2:\", list(data_2.keys()))\n",
        "\n",
        "# Assuming the correct keys for images and labels are different, update as needed\n",
        "images_1 = data_1['your_correct_images_key']\n",
        "labels_1 = data_1['your_correct_labels_key']\n",
        "images_2 = data_2['your_correct_images_key']\n",
        "labels_2 = data_2['your_correct_labels_key']\n",
        "\n",
        "# Combine the images and labels from both files\n",
        "combined_images = np.concatenate([images_1, images_2], axis=0)\n",
        "combined_labels = np.concatenate([labels_1, labels_2], axis=0)\n",
        "\n",
        "# Define the path for the combined data\n",
        "combined_file_path = '/content/drive/MyDrive/Capstone/Coral_images/data_augmented_com.npz'\n",
        "\n",
        "# Save the combined data to a new .npz file\n",
        "np.savez_compressed(combined_file_path, images=combined_images, labels=combined_labels)\n",
        "\n",
        "# Print a confirmation message\n",
        "print(f'Saved combined data to {combined_file_path}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "c2YQDqPgXk8l",
        "outputId": "7bbc56a2-a864-48c3-c625-b7c4c621ed62"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys in file 1: ['images', 'labels']\n",
            "Keys in file 2: ['images', 'labels']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-adc1c24f489e>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Assuming the correct keys for images and labels are different, update as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mimages_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'your_correct_images_key'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mlabels_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'your_correct_labels_key'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mimages_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'your_correct_images_key'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    258\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not a file in the archive\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'your_correct_images_key is not a file in the archive'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I83_5MWRZaL4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}